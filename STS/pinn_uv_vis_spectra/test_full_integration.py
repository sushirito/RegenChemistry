"""
Full Integration Tests for All 6 Phases of UV-Vis PINN Implementation
=====================================================================

This module tests the complete integration between all 6 phases of the 
UV-Vis spectroscopy PINN implementation, ensuring seamless data flow
and compatibility across the entire pipeline.

Test Coverage:
- Phase 1 â†’ Phase 2: Data preprocessing to model definition
- Phase 2 â†’ Phase 3: Model definition to loss functions  
- Phase 3 â†’ Phase 4: Loss functions to training pipeline
- Phase 4 â†’ Phase 5: Training to prediction pipeline
- Phase 5 â†’ Phase 6: Prediction to testing validation
- Complete end-to-end workflow integration

Author: Claude Code Assistant
"""

import pytest
import numpy as np
import pandas as pd
from pathlib import Path
import tempfile
import shutil
import json
import time
import warnings
from typing import Dict, Any, Tuple
import logging
from unittest.mock import Mock, patch

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Import all phases
try:
    # Phase 1: Data Preprocessing
    from data_preprocessing import UVVisDataProcessor, load_uvvis_data
    
    # Phase 2: Model Definition
    from model_definition import SpectroscopyPINN, create_spectroscopy_pinn
    
    # Phase 3: Loss Functions
    from loss_functions import UVVisLossFunction, create_uvvis_loss_function
    
    # Phase 4: Training
    from training import UVVisTrainingStrategy, UVVisLeaveOneScanOutCV, create_training_pipeline
    
    # Phase 5: Prediction
    from prediction import UVVisPredictionEngine, PredictionRequest, PredictionResult
    
    # Phase 6: Testing
    from test_comprehensive_suite import SyntheticDataGenerator
    
    ALL_PHASES_AVAILABLE = True
    
except ImportError as e:
    logger.warning(f"Some phases not available for integration testing: {e}")
    ALL_PHASES_AVAILABLE = False


# ============================================================================
# Integration Test Fixtures
# ============================================================================

@pytest.fixture(scope="module")
def integration_test_dir():
    """Create temporary directory for integration tests."""
    temp_dir = tempfile.mkdtemp(prefix="integration_tests_")
    yield Path(temp_dir)
    shutil.rmtree(temp_dir)


@pytest.fixture(scope="module")
def full_synthetic_dataset(integration_test_dir):
    """Generate comprehensive synthetic dataset for integration testing."""
    # Create realistic UV-Vis data with multiple features
    wavelengths = np.linspace(800, 200, 601)  # Descending order like real data
    concentrations = [0, 10, 20, 30, 40, 60]  # Î¼g/L
    
    def create_complex_spectrum(wl):
        """Create complex but realistic spectral features."""
        # Main plasmon resonance peak (gold nanoparticles)
        plasmon_main = 0.8 * np.exp(-((wl - 520) ** 2) / (2 * 40 ** 2))
        
        # Secondary plasmon feature
        plasmon_sec = 0.3 * np.exp(-((wl - 580) ** 2) / (2 * 60 ** 2))
        
        # UV absorption edge
        uv_edge = 0.4 * np.exp(-(wl - 250) / 50) * (wl < 350)
        
        # Scattering background (1/Î»â´ dependence)
        scattering = 0.02 * (400 / wl) ** 4 * (wl > 300)
        
        # Baseline absorption
        baseline = 0.03 + 0.01 * np.exp(-(wl - 200) / 100)
        
        return plasmon_main + plasmon_sec + uv_edge + scattering + baseline
    
    # Generate Beer-Lambert compliant data
    path_length = 1.0  # cm
    epsilon_spectrum = create_complex_spectrum(wavelengths)
    
    # Create CSV data
    data = {'Wavelength': wavelengths}
    
    for conc in concentrations:
        # A = Îµ Ã— b Ã— c + realistic noise
        absorbance = epsilon_spectrum * path_length * conc
        
        # Add correlated noise (more realistic than pure Gaussian)
        noise_level = 0.005  # 0.5% noise
        baseline_noise = np.random.normal(0, noise_level, len(wavelengths))
        
        # Smooth the noise (instrumental correlation)
        from scipy.ndimage import gaussian_filter1d
        smoothed_noise = gaussian_filter1d(baseline_noise, sigma=2)
        
        # Add shot noise (Poisson-like, intensity dependent)
        shot_noise = np.random.poisson(absorbance * 1000) / 1000 - absorbance
        shot_noise *= 0.001  # Scale down
        
        final_absorbance = absorbance + smoothed_noise + shot_noise
        
        data[str(conc)] = final_absorbance
    
    # Save to CSV
    df = pd.DataFrame(data)
    csv_path = integration_test_dir / "comprehensive_synthetic_data.csv"
    df.to_csv(csv_path, index=False)
    
    return {
        'csv_path': csv_path,
        'wavelengths': wavelengths,
        'concentrations': concentrations,
        'epsilon_true': epsilon_spectrum,
        'path_length': path_length,
        'noise_level': noise_level,
        'data_quality_metrics': {
            'snr_estimate': np.mean(epsilon_spectrum * np.max(concentrations)) / noise_level,
            'dynamic_range': np.max(epsilon_spectrum) / np.min(epsilon_spectrum[epsilon_spectrum > 0]),
            'spectral_complexity': len(wavelengths)
        }
    }


@pytest.fixture
def mock_deepxde_environment():
    """Mock DeepXDE environment for testing without dependencies."""
    with patch('deepxde.geometry.Rectangle') as mock_rect, \
         patch('deepxde.nn.FNN') as mock_fnn, \
         patch('deepxde.Model') as mock_model, \
         patch('deepxde.data.PDE') as mock_pde:
        
        # Configure mocks
        mock_rect.return_value = Mock()
        mock_fnn.return_value = Mock()
        mock_model.return_value = Mock()
        mock_pde.return_value = Mock()
        
        yield {
            'geometry': mock_rect,
            'network': mock_fnn,
            'model': mock_model,
            'pde': mock_pde
        }


# ============================================================================
# Phase-to-Phase Integration Tests
# ============================================================================

class TestPhase1To2Integration:
    """Test integration between Phase 1 (Data) and Phase 2 (Model)."""
    
    def test_data_model_dimension_compatibility(self, full_synthetic_dataset):
        """Test that data preprocessing output matches model input requirements."""
        # Phase 1: Process data
        csv_path = full_synthetic_dataset['csv_path']
        processor = UVVisDataProcessor(str(csv_path))
        processor.load_and_validate_data()
        processor.extract_spectral_components()
        processor.compute_beer_lambert_components()
        X_train, y_train = processor.create_training_data()
        
        # Phase 2: Create model
        pinn = SpectroscopyPINN()
        
        # Test compatibility
        assert X_train.shape[1] == 2, f"Expected 2 input features, got {X_train.shape[1]}"
        assert X_train.shape[1] == pinn.layer_sizes[0], "Model input dimension mismatch"
        assert y_train.shape[1] == 1, f"Expected 1 output feature, got {y_train.shape[1]}"
        
        # Test data ranges
        wl_norm, conc_norm = X_train[:, 0], X_train[:, 1]
        assert np.all(np.abs(wl_norm) <= 1.1), f"Wavelength normalization out of range: [{wl_norm.min()}, {wl_norm.max()}]"
        assert np.all(conc_norm >= -0.1) and np.all(conc_norm <= 1.1), f"Concentration normalization out of range: [{conc_norm.min()}, {conc_norm.max()}]"
    
    def test_domain_parameter_consistency(self, full_synthetic_dataset):
        """Test that model domain parameters match data ranges."""
        # Phase 1: Extract data ranges
        csv_path = full_synthetic_dataset['csv_path']
        processor = UVVisDataProcessor(str(csv_path))
        processor.load_and_validate_data()
        processor.extract_spectral_components()
        
        data_wl_range = (processor.wavelengths.min(), processor.wavelengths.max())
        data_conc_range = (processor.concentrations.min(), processor.concentrations.max())
        
        # Phase 2: Create model with matching domain
        pinn = SpectroscopyPINN(
            wavelength_range=data_wl_range,
            concentration_range=data_conc_range
        )
        
        # Test consistency
        assert pinn.wavelength_range == data_wl_range
        assert pinn.concentration_range == data_conc_range
        assert pinn.path_length == full_synthetic_dataset['path_length']
    
    def test_physics_parameter_alignment(self, full_synthetic_dataset):
        """Test that physics parameters are consistent between data and model."""
        # Phase 1: Process data
        csv_path = full_synthetic_dataset['csv_path']
        processor = UVVisDataProcessor(str(csv_path))
        processor.load_and_validate_data()
        processor.extract_spectral_components()
        processor.compute_beer_lambert_components()
        
        # Phase 2: Create model
        pinn = SpectroscopyPINN()
        
        # Test Beer-Lambert parameter consistency
        expected_path_length = full_synthetic_dataset['path_length']
        assert pinn.path_length == expected_path_length
        
        # Test that differential absorption was computed correctly
        baseline = processor.baseline_absorption
        differential = processor.differential_absorption
        
        assert np.allclose(baseline, processor.absorbance_matrix[:, 0])
        assert differential.shape[1] == len(processor.concentrations_nonzero)


class TestPhase2To3Integration:
    """Test integration between Phase 2 (Model) and Phase 3 (Loss)."""
    
    @pytest.mark.skipif(not ALL_PHASES_AVAILABLE, reason="Not all phases available")
    def test_model_loss_function_compatibility(self, mock_deepxde_environment):
        """Test that model architecture is compatible with loss function."""
        # Phase 2: Create model
        pinn = SpectroscopyPINN()
        
        # Phase 3: Create loss function
        try:
            loss_fn = create_uvvis_loss_function(
                path_length=pinn.path_length,
                physics_weight=0.1,
                smooth_weight=1e-4
            )
            
            assert callable(loss_fn)
            
            # Test that loss function parameters match model
            # This would require access to the UVVisLossFunction object
            # In practice, we'd verify path_length consistency
            
        except ImportError:
            pytest.skip("TensorFlow not available for loss function testing")
    
    def test_physics_constraint_consistency(self, full_synthetic_dataset):
        """Test that physics constraints are consistent between model and loss."""
        # Phase 2: Create model
        pinn = SpectroscopyPINN()
        
        # Phase 3: Verify Beer-Lambert parameters
        expected_path_length = full_synthetic_dataset['path_length']
        
        assert pinn.path_length == expected_path_length
        
        # In a real implementation, we'd test that:
        # 1. Loss function uses same path length
        # 2. Physics constraints match model domain
        # 3. Gradient computation is consistent


class TestPhase3To4Integration:
    """Test integration between Phase 3 (Loss) and Phase 4 (Training)."""
    
    def test_loss_training_pipeline_compatibility(self, full_synthetic_dataset, mock_deepxde_environment):
        """Test that loss functions work with training pipeline."""
        # Phase 1: Prepare data
        csv_path = full_synthetic_dataset['csv_path']
        processor = UVVisDataProcessor(str(csv_path))
        processor.load_and_validate_data()
        processor.extract_spectral_components()
        processor.compute_beer_lambert_components()
        X_train, y_train = processor.create_training_data()
        
        # Phase 2: Create model
        pinn = SpectroscopyPINN()
        
        # Mock model creation for testing
        mock_model = Mock()
        mock_model.compile = Mock()
        mock_model.train = Mock()
        
        # Phase 3: Create loss function (mock)
        mock_loss_fn = Mock()
        
        # Phase 4: Test training strategy initialization
        training_strategy = UVVisTrainingStrategy(
            model=mock_model,
            loss_function=mock_loss_fn
        )
        
        assert training_strategy.model == mock_model
        assert training_strategy.loss_function == mock_loss_fn
    
    def test_cross_validation_data_compatibility(self, full_synthetic_dataset):
        """Test that CV strategy works with preprocessed data."""
        # Phase 1: Process data
        csv_path = full_synthetic_dataset['csv_path']
        processor = UVVisDataProcessor(str(csv_path))
        processor.load_and_validate_data()
        processor.extract_spectral_components()
        processor.compute_beer_lambert_components()
        
        # Phase 4: Test CV strategy
        def mock_model_factory(data):
            return Mock()
        
        def mock_loss_factory():
            return Mock()
        
        cv_strategy = UVVisLeaveOneScanOutCV(
            data_processor=processor,
            model_factory=mock_model_factory,
            loss_factory=mock_loss_factory
        )
        
        # Test that CV strategy can access data
        assert len(cv_strategy.concentration_levels) == len(processor.concentrations_nonzero)
        assert np.array_equal(cv_strategy.concentration_levels, processor.concentrations_nonzero)


class TestPhase4To5Integration:
    """Test integration between Phase 4 (Training) and Phase 5 (Prediction)."""
    
    def test_training_prediction_model_handoff(self, full_synthetic_dataset):
        """Test that trained models can be used for prediction."""
        # Phase 1: Process data
        csv_path = full_synthetic_dataset['csv_path']
        processor = UVVisDataProcessor(str(csv_path))
        processor.load_and_validate_data()
        processor.extract_spectral_components()
        processor.compute_beer_lambert_components()
        
        # Mock trained model
        mock_model = Mock()
        mock_model.predict = lambda x: np.random.rand(len(x), 1) * 0.1  # Reasonable values
        
        # Phase 5: Test prediction engine
        engine = UVVisPredictionEngine(
            model=mock_model,
            data_processor=processor
        )
        
        assert engine.model == mock_model
        assert engine.data_processor == processor
    
    def test_prediction_data_format_consistency(self, full_synthetic_dataset):
        """Test that prediction engine handles data format correctly."""
        # Phase 1: Process data
        csv_path = full_synthetic_dataset['csv_path']
        processor = UVVisDataProcessor(str(csv_path))
        processor.load_and_validate_data()
        processor.extract_spectral_components()
        processor.compute_beer_lambert_components()
        
        # Mock model with consistent output format
        def mock_predict(inputs):
            # Return predictions matching expected format
            return np.random.rand(len(inputs), 1) * 0.1
        
        mock_model = Mock()
        mock_model.predict = mock_predict
        
        # Phase 5: Test prediction
        engine = UVVisPredictionEngine(
            model=mock_model,
            data_processor=processor
        )
        
        request = PredictionRequest(
            concentrations=25.0,
            include_uncertainty=False,
            include_physics_validation=False
        )
        
        result = engine.predict(request)
        
        # Test result format
        assert isinstance(result, PredictionResult)
        assert len(result.concentrations) == 1
        assert result.concentrations[0] == 25.0
        assert result.predicted_spectra.shape[1] == 1
        assert result.predicted_spectra.shape[0] == len(processor.wavelengths)


class TestPhase5To6Integration:
    """Test integration between Phase 5 (Prediction) and Phase 6 (Testing)."""
    
    def test_prediction_validation_compatibility(self, full_synthetic_dataset):
        """Test that prediction results can be validated by physics tests."""
        # Phase 1: Process data
        csv_path = full_synthetic_dataset['csv_path']
        processor = UVVisDataProcessor(str(csv_path))
        processor.load_and_validate_data()
        processor.extract_spectral_components()
        processor.compute_beer_lambert_components()
        
        # Create realistic mock predictions
        concentrations = [10.0, 20.0, 30.0, 40.0]
        n_wl = len(processor.wavelengths)
        
        # Generate predictions that should follow Beer-Lambert law
        epsilon_mock = full_synthetic_dataset['epsilon_true']
        predictions = np.outer(epsilon_mock, concentrations)
        
        # Phase 5: Create prediction result
        result = PredictionResult(
            concentrations=np.array(concentrations),
            wavelengths=processor.wavelengths,
            predicted_spectra=predictions,
            baseline_spectrum=processor.baseline_absorption
        )
        
        # Phase 6: Validate using physics tests
        from prediction import BeerLambertValidator
        validator = BeerLambertValidator()
        
        validation_results = validator.validate_predictions(
            result.concentrations,
            result.wavelengths,
            result.predicted_spectra,
            result.baseline_spectrum
        )
        
        # Should pass validation for well-behaved synthetic data
        assert validation_results['overall_compliance'] is True
        assert validation_results['linearity_test']['passed'] is True
        assert validation_results['linearity_test']['mean_r2'] > 0.95


# ============================================================================
# Complete End-to-End Integration Tests
# ============================================================================

class TestCompleteWorkflowIntegration:
    """Test complete end-to-end workflow integration."""
    
    def test_phase_1_through_6_synthetic_workflow(self, full_synthetic_dataset, integration_test_dir):
        """Test complete workflow from data to validation with synthetic data."""
        logger.info("Starting complete Phase 1-6 integration test...")
        
        workflow_results = {}
        
        # Phase 1: Data Preprocessing
        logger.info("Phase 1: Data Preprocessing")
        csv_path = full_synthetic_dataset['csv_path']
        
        processor = UVVisDataProcessor(str(csv_path))
        processor.load_and_validate_data()
        processor.extract_spectral_components()
        processor.compute_beer_lambert_components()
        X_train, y_train = processor.create_training_data()
        
        workflow_results['phase_1'] = {
            'data_shape': X_train.shape,
            'target_shape': y_train.shape,
            'wavelength_range': (processor.wavelengths.min(), processor.wavelengths.max()),
            'concentration_range': (processor.concentrations.min(), processor.concentrations.max())
        }
        
        logger.info(f"Phase 1 completed: {X_train.shape[0]} training samples")
        
        # Phase 2: Model Definition
        logger.info("Phase 2: Model Definition")
        pinn = create_spectroscopy_pinn(
            wavelength_range=workflow_results['phase_1']['wavelength_range'],
            concentration_range=workflow_results['phase_1']['concentration_range'],
            architecture="standard"
        )
        
        model_summary = pinn.get_model_summary()
        workflow_results['phase_2'] = {
            'architecture': model_summary['architecture'],
            'parameter_count': model_summary['architecture']['total_parameters'],
            'domain_consistency': (pinn.wavelength_range == workflow_results['phase_1']['wavelength_range'])
        }
        
        logger.info(f"Phase 2 completed: {workflow_results['phase_2']['parameter_count']} parameters")
        
        # Phase 3: Loss Functions (interface test)
        logger.info("Phase 3: Loss Functions")
        try:
            loss_fn = create_uvvis_loss_function(
                path_length=pinn.path_length,
                physics_weight=0.1,
                smooth_weight=1e-4
            )
            workflow_results['phase_3'] = {
                'loss_function_created': callable(loss_fn),
                'path_length_consistent': True
            }
            logger.info("Phase 3 completed: Loss function interface validated")
        except ImportError:
            workflow_results['phase_3'] = {
                'loss_function_created': False,
                'skip_reason': 'TensorFlow not available'
            }
            logger.info("Phase 3 skipped: TensorFlow dependencies not available")
        
        # Phase 4: Training (mock implementation)
        logger.info("Phase 4: Training Pipeline")
        
        # Create mock trained model with realistic behavior
        def create_mock_trained_model():
            mock_model = Mock()
            
            def realistic_predict(inputs):
                # Extract normalized wavelength and concentration
                wl_norm = inputs[:, 0]
                conc_norm = inputs[:, 1]
                
                # Use actual epsilon spectrum for realistic predictions
                epsilon_true = full_synthetic_dataset['epsilon_true']
                
                # Denormalize wavelength to get epsilon values
                wl_denorm = processor.denormalize_wavelength(wl_norm)
                
                # Interpolate epsilon values
                epsilon_pred = np.interp(wl_denorm, processor.wavelengths, epsilon_true)
                
                # Apply Beer-Lambert law
                conc_denorm = processor.denormalize_concentration(conc_norm)
                absorption_pred = epsilon_pred * pinn.path_length * conc_denorm
                
                return absorption_pred.reshape(-1, 1)
            
            mock_model.predict = realistic_predict
            return mock_model
        
        trained_model = create_mock_trained_model()
        workflow_results['phase_4'] = {
            'model_trained': True,
            'training_method': 'mock_with_realistic_behavior'
        }
        
        logger.info("Phase 4 completed: Mock training with realistic model behavior")
        
        # Phase 5: Prediction
        logger.info("Phase 5: Prediction")
        engine = UVVisPredictionEngine(
            model=trained_model,
            data_processor=processor
        )
        
        # Test single prediction
        single_request = PredictionRequest(
            concentrations=25.0,
            include_uncertainty=False,
            include_physics_validation=True
        )
        
        single_result = engine.predict(single_request)
        
        # Test batch predictions
        batch_request = PredictionRequest(
            concentrations=[15.0, 25.0, 35.0, 45.0],
            include_uncertainty=False,
            include_physics_validation=True
        )
        
        batch_result = engine.predict(batch_request)
        
        workflow_results['phase_5'] = {
            'single_prediction_shape': single_result.predicted_spectra.shape,
            'batch_prediction_shape': batch_result.predicted_spectra.shape,
            'physics_validation_passed': (
                single_result.physics_validation['overall_compliance'] and
                batch_result.physics_validation['overall_compliance']
            )
        }
        
        logger.info(f"Phase 5 completed: Predictions for {len(batch_result.concentrations)} concentrations")
        
        # Phase 6: Validation and Testing
        logger.info("Phase 6: Testing and Validation")
        
        # Test physics compliance
        physics_results = {
            'single_compliance': single_result.physics_validation['overall_compliance'],
            'batch_compliance': batch_result.physics_validation['overall_compliance'],
            'linearity_scores': {
                'single': single_result.physics_validation['linearity_test']['mean_r2'],
                'batch': batch_result.physics_validation['linearity_test']['mean_r2']
            }
        }
        
        # Test prediction accuracy against known synthetic truth
        accuracy_metrics = self._compute_prediction_accuracy(
            batch_result, full_synthetic_dataset, processor
        )
        
        workflow_results['phase_6'] = {
            'physics_validation': physics_results,
            'accuracy_metrics': accuracy_metrics,
            'overall_test_passed': (
                physics_results['single_compliance'] and 
                physics_results['batch_compliance'] and
                accuracy_metrics['mean_relative_error'] < 0.1  # 10% error threshold
            )
        }
        
        logger.info("Phase 6 completed: Validation and testing")
        
        # Save workflow results
        results_path = integration_test_dir / "complete_workflow_results.json"
        with open(results_path, 'w') as f:
            # Make results JSON serializable
            serializable_results = self._make_json_serializable(workflow_results)
            json.dump(serializable_results, f, indent=2)
        
        logger.info(f"Workflow results saved to {results_path}")
        
        # Final assertions
        assert workflow_results['phase_1']['data_shape'][0] > 0, "No training data generated"
        assert workflow_results['phase_2']['parameter_count'] > 0, "No model parameters"
        assert workflow_results['phase_5']['physics_validation_passed'], "Physics validation failed"
        assert workflow_results['phase_6']['overall_test_passed'], "Overall testing failed"
        
        logger.info("âœ… Complete Phase 1-6 integration test PASSED")
        
        return workflow_results
    
    def _compute_prediction_accuracy(self, result, synthetic_data, processor):
        """Compute prediction accuracy against synthetic ground truth."""
        # Get ground truth epsilon
        epsilon_true = synthetic_data['epsilon_true']
        
        # Extract predictions and compute epsilon estimates
        concentrations = result.concentrations
        predictions = result.predicted_spectra
        
        # Convert predictions back to epsilon estimates
        epsilon_estimates = []
        for i, conc in enumerate(concentrations):
            if conc > 0:  # Avoid division by zero
                epsilon_est = predictions[:, i] / (synthetic_data['path_length'] * conc)
                epsilon_estimates.append(epsilon_est)
        
        if epsilon_estimates:
            epsilon_pred_mean = np.mean(epsilon_estimates, axis=0)
            
            # Compute accuracy metrics
            abs_error = np.abs(epsilon_pred_mean - epsilon_true)
            rel_error = abs_error / (epsilon_true + 1e-10)
            
            return {
                'mean_absolute_error': float(np.mean(abs_error)),
                'mean_relative_error': float(np.mean(rel_error)),
                'max_relative_error': float(np.max(rel_error)),
                'correlation': float(np.corrcoef(epsilon_pred_mean, epsilon_true)[0, 1])
            }
        else:
            return {'error': 'No non-zero concentrations for accuracy computation'}
    
    def _make_json_serializable(self, obj):
        """Convert objects to JSON-serializable format."""
        if isinstance(obj, dict):
            return {k: self._make_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_json_serializable(item) for item in obj]
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.float32, np.float64, np.int32, np.int64)):
            return float(obj) if 'float' in str(type(obj)) else int(obj)
        elif isinstance(obj, tuple):
            return list(obj)
        else:
            return obj
    
    @pytest.mark.skipif(not Path("/Users/aditya/CodingProjects/datasets/0.30MB_AuNP_As.csv").exists(),
                       reason="Real dataset not available")
    def test_real_data_integration_workflow(self, integration_test_dir):
        """Test integration with real UV-Vis dataset."""
        logger.info("Starting real data integration test...")
        
        real_data_path = Path("/Users/aditya/CodingProjects/datasets/0.30MB_AuNP_As.csv")
        
        # Phase 1: Process real data
        processor = UVVisDataProcessor(str(real_data_path))
        processor.load_and_validate_data()
        processor.extract_spectral_components()
        processor.compute_beer_lambert_components()
        X_train, y_train = processor.create_training_data()
        
        # Verify expected real data dimensions
        assert X_train.shape == (3005, 2), f"Unexpected real data shape: {X_train.shape}"
        
        # Phase 2: Create model for real data
        pinn = SpectroscopyPINN(
            wavelength_range=(200, 800),
            concentration_range=(0, 60)
        )
        
        # Phase 5: Test prediction with real data structure (mock model)
        mock_model = Mock()
        mock_model.predict = lambda x: np.random.rand(len(x), 1) * 0.01  # Small realistic values
        
        engine = UVVisPredictionEngine(
            model=mock_model,
            data_processor=processor
        )
        
        # Test prediction on intermediate concentrations
        request = PredictionRequest(
            concentrations=[5.0, 15.0, 25.0, 35.0, 45.0, 55.0],
            include_physics_validation=True
        )
        
        result = engine.predict(request)
        
        # Verify result structure
        assert result.predicted_spectra.shape == (601, 6)  # 601 wavelengths, 6 concentrations
        assert len(result.wavelengths) == 601
        assert np.array_equal(result.concentrations, [5.0, 15.0, 25.0, 35.0, 45.0, 55.0])
        
        # Save real data test results
        results_path = integration_test_dir / "real_data_integration_results.json"
        with open(results_path, 'w') as f:
            json.dump({
                'data_shape': X_train.shape,
                'prediction_shape': result.predicted_spectra.shape,
                'physics_validation': result.physics_validation,
                'test_status': 'passed'
            }, f, indent=2, default=str)
        
        logger.info("âœ… Real data integration test PASSED")


# ============================================================================
# Performance and Stress Testing
# ============================================================================

class TestIntegrationPerformance:
    """Test performance characteristics of integrated system."""
    
    def test_end_to_end_performance(self, full_synthetic_dataset):
        """Test end-to-end performance of complete pipeline."""
        start_time = time.time()
        
        # Phase 1: Data processing
        csv_path = full_synthetic_dataset['csv_path']
        processor = UVVisDataProcessor(str(csv_path))
        processor.load_and_validate_data()
        processor.extract_spectral_components()
        processor.compute_beer_lambert_components()
        X_train, y_train = processor.create_training_data()
        
        phase1_time = time.time()
        
        # Phase 2: Model creation
        pinn = create_spectroscopy_pinn()
        
        phase2_time = time.time()
        
        # Phase 5: Prediction (with mock model)
        mock_model = Mock()
        mock_model.predict = lambda x: np.random.rand(len(x), 1) * 0.1
        
        engine = UVVisPredictionEngine(
            model=mock_model,
            data_processor=processor
        )
        
        # Batch prediction
        request = PredictionRequest(
            concentrations=list(range(1, 51)),  # 50 concentrations
            include_uncertainty=False,
            include_physics_validation=True
        )
        
        result = engine.predict(request)
        
        end_time = time.time()
        
        # Performance metrics
        total_time = end_time - start_time
        data_processing_time = phase1_time - start_time
        model_creation_time = phase2_time - phase1_time
        prediction_time = end_time - phase2_time
        
        # Performance requirements
        assert total_time < 10.0, f"End-to-end pipeline too slow: {total_time:.2f}s"
        assert data_processing_time < 5.0, f"Data processing too slow: {data_processing_time:.2f}s"
        assert prediction_time < 5.0, f"Batch prediction too slow: {prediction_time:.2f}s"
        
        logger.info(f"Performance test passed - Total: {total_time:.2f}s, "
                   f"Data: {data_processing_time:.2f}s, Prediction: {prediction_time:.2f}s")
    
    def test_memory_efficiency(self, full_synthetic_dataset):
        """Test memory efficiency of integrated system."""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # Process large dataset
        csv_path = full_synthetic_dataset['csv_path']
        processor = UVVisDataProcessor(str(csv_path))
        processor.load_and_validate_data()
        processor.extract_spectral_components()
        processor.compute_beer_lambert_components()
        X_train, y_train = processor.create_training_data()
        
        # Create multiple models
        models = [create_spectroscopy_pinn() for _ in range(5)]
        
        # Multiple predictions
        mock_model = Mock()
        mock_model.predict = lambda x: np.random.rand(len(x), 1) * 0.1
        
        engine = UVVisPredictionEngine(
            model=mock_model,
            data_processor=processor
        )
        
        results = []
        for i in range(10):
            request = PredictionRequest(
                concentrations=list(range(i*5, (i+1)*5)),
                include_uncertainty=False
            )
            result = engine.predict(request)
            results.append(result)
        
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - initial_memory
        
        # Memory should not increase excessively
        assert memory_increase < 500, f"Excessive memory usage: {memory_increase:.1f}MB increase"
        
        logger.info(f"Memory efficiency test passed - Memory increase: {memory_increase:.1f}MB")


# ============================================================================
# Error Recovery and Robustness Testing
# ============================================================================

class TestIntegrationRobustness:
    """Test robustness and error recovery of integrated system."""
    
    def test_phase_failure_recovery(self, full_synthetic_dataset):
        """Test system recovery from phase failures."""
        csv_path = full_synthetic_dataset['csv_path']
        
        # Test recovery from data processing failure
        with patch.object(UVVisDataProcessor, 'load_and_validate_data', side_effect=Exception("Mock failure")):
            with pytest.raises(Exception):
                processor = UVVisDataProcessor(str(csv_path))
                processor.load_and_validate_data()
        
        # System should still work after exception
        processor = UVVisDataProcessor(str(csv_path))
        processor.load_and_validate_data()  # Should work normally
        
        assert processor.raw_data is not None
    
    def test_data_quality_handling(self, integration_test_dir):
        """Test handling of various data quality issues."""
        # Create data with quality issues
        problematic_data = {
            'Wavelength': [800, 700, 600, 500, 400],
            '0': [0.1, 0.2, np.nan, 0.4, 0.5],  # Missing value
            '10': [0.15, 0.25, 0.35, -0.1, 0.55],  # Negative value
            '20': [0.2, 0.3, 0.4, 0.5, 10.0]  # Extreme value
        }
        
        df = pd.DataFrame(problematic_data)
        problematic_csv = integration_test_dir / "problematic_data.csv"
        df.to_csv(problematic_csv, index=False)
        
        # Test that system handles problematic data gracefully
        processor = UVVisDataProcessor(str(problematic_csv), validate_data=True)
        
        with pytest.raises(ValueError):  # Should detect missing values
            processor.load_and_validate_data()
    
    def test_prediction_edge_cases(self, full_synthetic_dataset):
        """Test prediction with edge case inputs."""
        # Setup
        csv_path = full_synthetic_dataset['csv_path']
        processor = UVVisDataProcessor(str(csv_path))
        processor.load_and_validate_data()
        processor.extract_spectral_components()
        processor.compute_beer_lambert_components()
        
        mock_model = Mock()
        mock_model.predict = lambda x: np.random.rand(len(x), 1) * 0.1
        
        engine = UVVisPredictionEngine(
            model=mock_model,
            data_processor=processor
        )
        
        # Test edge cases
        edge_cases = [
            0.0,      # Zero concentration
            0.001,    # Very small concentration
            1000.0,   # Very large concentration
            -5.0      # Negative concentration (should handle gracefully)
        ]
        
        for conc in edge_cases:
            try:
                request = PredictionRequest(
                    concentrations=conc,
                    include_uncertainty=False,
                    include_physics_validation=False
                )
                result = engine.predict(request)
                
                # Should produce some result
                assert result is not None
                assert result.predicted_spectra is not None
                
            except Exception as e:
                # Log but don't fail - some edge cases may legitimately fail
                logger.warning(f"Edge case {conc} failed: {e}")


# ============================================================================
# Test Execution
# ============================================================================

def run_integration_tests():
    """Run all integration tests with comprehensive reporting."""
    pytest_args = [
        __file__,
        "-v",
        "--tb=short",
        "-x",  # Stop on first failure for integration tests
    ]
    
    logger.info("Running comprehensive integration tests...")
    result = pytest.main(pytest_args)
    
    if result == 0:
        logger.info("ðŸŽ‰ All integration tests passed! The complete 6-phase UV-Vis PINN implementation is working correctly.")
    else:
        logger.error("âŒ Some integration tests failed. Please check the implementation.")
    
    return result


if __name__ == "__main__":
    exit_code = run_integration_tests()
    exit(exit_code)